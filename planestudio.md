# Plan de estudios para Data Enginerring  
  
## Módulo 1: Fundamentos de programación (Python)
##### Duración 4 - 6 semanas
  
[ ] Sintaxis básica de python  
[ ] Tipos de datos y estrucuras (listas, sets, diccionarios, tuplas)  
[ ] Funciones y módulos  
[ ] Manejo de errores  
[ ] Lectura y escritura de archivos  
[ ] Introducción a pandas  
[ ] Uso de entornos virtuales (venv, pip)  

### Proyectos  
  
- [Limpieza de un data set de ventas]()
- [Análisis básico de un dataset de clientes]()
- [Automatización para descargar y guardar datos diariamente]()
- [Ordenador automático de archivos]()
  
## Módulo 2: SQL Avanzado + Modelado de Datos
##### Duración 6 - 8 semanas
  
[ ] SELECT, JOIN, GROUPBY  
[ ] Subqueries y CETs  
[ ] Windows functions (muy importantes)
[ ] Optimización de consultas  
[ ] Normalización (1NF, 2NF, 3NF)  
[ ] Diseño estrella y copo de nieve (kimball)  

### Proyectos
- [Sistema de ventas con modelos estrella + consultas avanzadas]()
- [Análsis de KPIs financieros con SQL avanzado]()
- [Optimización de consultas en una base de datos de producción]()
- [Data Mart para un negoció real con SQL + diseño lógico]()
  
## Módulo 3: Bases de datos SQL y NoSQL  
##### Duración 3 - 4 semanas  
  
[ ] Bases relacionales PostgeSQL, MySQL  
[ ] NoSQL MongoDB, Redis  
[ ] Índices, particiones, replicación (conceptos), diferenteis entre OLTP vs OLAP  
  
### Proyectos  
- [Sistema de gestión de inventario SQL - PostgreSQL]()
- [Análisis de reviews de productos NoSQL - MongoDB]()
- [Sistema de cacheo de sesiones NoSQL - Redis]()
- [Modelos híbrido SQL + NoSQL PostgreSQL + MongoDB]()  
  
## Módulo 4: Procesamiento de datos: Spark, PySpark y Big Data 
##### Duración 8 - 10 semanas  
  
[ ] ¿Qué es un cluster?  
[ ] Spark: estructura y arquitectura  
[ ] RDDs vs DataFrames  
[ ] Jobs, stages, tasks  
[ ] PySpark (transformations, actions)  
[ ] Optimización con catalyst y tungsten  
[ ] Esquema de datos (parquet, avro, ORC) 
  
### Proyectos  
- [Pipline ETL en PySpark con archivos parquet]()  
- [Análisis de grandes volúmenes usando PySpark + SQL]()
- [Pipeline de enriquecimiento de datos en spark (joins de grandes datasets)]()
- [Procesamiento distribuido de logs con pyspark + json]()
